---
title: "Estudo de Caso - Diagnóstico de Câncer de Mama"
author: "Bruno Crotman"
date: "25/06/2019"
header-includes:
- \usepackage[portuguese]{babel}
output: 
    pdf_document:
        number_sections: true
        fig_caption: yes

bibliography: bib.bib           
        

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)


library(caret)
library(tidyverse)
library(scales)
library(GGally)
library(pROC)
library(plotROC)
library(xtable)
library(knitr)


dados <- read_csv("wdbc.csv") %>% 
    select(-"ID number") %>% 
    rename_all( .funs = ~str_replace(.,"fractal-media ","fractal_")   ) %>% 
    rename_all( .funs = ~str_replace(.,"fractal-pior ","fractal_")   ) %>% 
    rename_all( .funs = ~str_replace(.,"fractal-dv ","fractal_")   ) %>% 
    rename("fractal_dimension-media" = "fractal_dimension-meia") %>% 
    rename_all( .funs = ~str_replace(.,"-","_")   ) %>% 
    rename_all( .funs = ~str_replace(.," ","_")   ) %>% 
    mutate(Diagnosis = as.factor(Diagnosis))


set.seed(13)


rows <- sample(nrow(dados))

dados <- dados[rows,]

split <- round(nrow(dados) * .70 )

treino <- dados[1:split, ]

teste <- dados[(split + 1):nrow(dados), ]





```



```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(texreg)
library(xtable)
library(broom)

options(OutDec = ",")

```



\section{Origem dos dados}

Os dados utilizados vieram do site [UCI Machine Learning Repository - Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))

A base contém `r nrow(dados)`, com `r ncol(dados)` variáveis, sendo uma delas o diagnóstico de benigno ou maligno.

\section{Conjuntos Treino e Teste}


Já antes o início da análise, a base foi dividida aleatoriamente em dados de teste e dados de treino. Os dados de teste não foram tocados durante nenhuma parte da análise. Apenas na avaliação final dos modelos.

Os dados de treino e teste constituem, respectivamente, `r    percent(nrow(treino)/nrow(dados), decimal.mark = ",", accuracy = 1)` e `r percent(nrow(teste)/nrow(dados) , decimal.mark = ",", accuracy = 1 )` do total.



```{r message=FALSE, warning=FALSE, paged.print=FALSE}




medias <- treino %>% 
    select_at(vars(ends_with("_media"),Diagnosis)) %>% 
    rename_all( .funs = ~str_replace(.,"_media","")   ) %>% 
    rename_all( .funs = ~str_replace(.," ","_")   ) 
    


piores <- treino %>% 
    select_at(vars(ends_with("_pior"),Diagnosis)) %>% 
    rename_all( .funs = ~str_replace(.,"_pior","")   )  %>% 
    rename_all( .funs = ~str_replace(.," ","_")   ) 


dps <- treino %>% 
    select_at(vars(ends_with("_dv"),Diagnosis)) %>% 
    rename_all( .funs = ~str_replace(.,"_dv","")   ) %>% 
    rename_all( .funs = ~str_replace(.," ","_")   )  


```



```{r cache= TRUE}

ggcorr(medias %>% select(-Diagnosis), legend.size = 8) +
    ggtitle("Correlações", subtitle = "É possível observar que há muitas variáveis bastante \n correlacionadas")

```



```{r message=FALSE, warning=FALSE, paged.print=FALSE ,cache= TRUE}

ggpairs(medias ) +
    theme_grey(base_size = 8) +
        ggtitle("Resumo das distribuições", subtitle = "É possível observar que o valor da maioria das variáveis se mostra maior para os resultados malignos")



```



```{r}
ggcorr(piores %>% select(-Diagnosis), legend.size = 8) +
    ggtitle("Correlações", subtitle = "É possível observar que há muitas variáveis bastante \n correlacionadas")

```




```{r message=FALSE, warning=FALSE, paged.print=FALSE, cache= TRUE}
ggpairs(piores ) +
    theme_grey(base_size = 8) +
        ggtitle("Resumo das distribuições", subtitle = "É possível observar que o valor da maioria das variáveis se mostra maior para os resultados malignos")

```


```{r}


ggcorr(dps %>% select(-Diagnosis), legend.size = 8) +
    ggtitle("Correlações", subtitle = "É possível observar que há muitas variáveis bastante \n correlacionadas")


```





```{r message=FALSE, warning=FALSE, paged.print=FALSE , cache= TRUE}

ggpairs(dps ) +
    theme_grey(base_size = 8) +
        ggtitle("Resumo das distribuições", subtitle = "É possível observar que o valor da maioria das variáveis se mostra maior para os resultados malignos")


```

\section{Modelos de classificação}



O modelo é testado em esquema 10-Fold-validation

```{r}


folds <- createFolds(treino$Diagnosis, k = 10 )

controle <- trainControl(
    
    summaryFunction = twoClassSummary,
    classProbs = TRUE,
    verboseIter = FALSE,
    index = folds, 
    returnResamp = "all",
    returnData = TRUE,
    savePredictions = "all",
    
    
    
)


```



\subsection{Regressão logística}



Em seguida é mostrado o resultado da regressão logística sem o tratamento de redução de dimensionalidade PCA.




```{r message=FALSE, warning=FALSE, paged.print=FALSE}

model_logistic <- train(
    
    Diagnosis ~ .,
    treino,
    metric = "ROC",
    method = "glm",
    trControl = controle,
    preProcess = c( "center", "scale"),
    family=binomial(link='logit')
    

)



ggplot(model_logistic$pred, aes(m = M, d = obs, color = Resample )) +
    geom_roc( labels = FALSE  ) +
    coord_equal() + style_roc() + ggtitle("ROC", subtitle = "Métricas para diversos thresholds" )



```





```{r}


model_logistic$resample %>% 
    select(-parameter) %>%
    mutate_if(is.numeric, percent) %>% 
    kable( caption = "Métricas para cada Fold")



```


```{r}


model_logistic$resample %>%
    select(-parameter) %>% 
    gather(metrica, valor, -Resample) %>% 
    group_by(metrica) %>% 
    summarise(media = mean(valor), sd = sd(valor)) %>% 
    rename("Métrica" = metrica, "Média" = media, "Desvio-padrão" = sd) %>% 
    mutate_if(is.numeric, percent) %>% 
    kable()
    


```




```{r}


summary(model_logistic$finalModel)


```


```{r}

correlacoes <- cor(as.matrix(treino %>% select(-Diagnosis)))

correlacoes_df <- as_tibble(correlacoes) 

correlacoes_df <- correlacoes_df %>% 
    bind_cols(tibble(coluna1 = names(correlacoes_df))) %>% 
    gather( coluna2, correlacao, -coluna1  ) %>% 
    filter(coluna1 != coluna2) %>% 
    arrange(desc(correlacao)) %>% 
    mutate(coluna1_menor = coluna1 < coluna2) %>% 
    filter(coluna1_menor) %>% 
    select(-coluna1_menor)

pares_avaliacao <- correlacoes_df %>% 
    mutate(par = row_number()) %>% 
    gather(variavel, valor,-correlacao, -par  ) %>% 
    mutate(formula =paste0("Diagnosis ~ . -", valor)) %>% 
    filter(correlacao > 0.99 ) %>% 
    rowwise() %>% 
    mutate( ROC = c(1,2)  ) %T>% 
    View()
    

    
    
    

media_roc_formula <- function(formula, dados)
{
    model_logistic <- train(
        
        as.formula(formula),
        dados,
        metric = "ROC",
        method = "glm",
        trControl = controle,
        preProcess = c( "center", "scale"),
        family=binomial(link='logit')

    )
    
    retorno <- model_logistic$resample %>%
        select(-parameter) %>% 
        gather(metrica, valor, -Resample) %>% 
        group_by(metrica) %>% 
        summarise(media = mean(valor)) %>% 
        filter(metrica == "ROC") %>% 
        pull(media)
        

    
    list(retorno,2)
    
    
        
}


    
    
    


```

```{r}


var <-  "radius_media"

model_logistic <- train(
    
    as.formula("Diagnosis ~ . -radius_media"),
    treino,
    metric = "ROC",
    method = "glm",
    trControl = controle,
    preProcess = c( "center", "scale"),
    family=binomial(link='logit')
    

)

summary(model_logistic$finalModel)

```



<!-- \section{Algoritmo Gale-Shapley} -->


<!-- O algoritmo implementado foi baseado no livro-texto da matéria [@tardos2006algorithm]. Basicamente o algoritmo segue os seguintes passos, usando um vocabulário de casamentos entre heterossexuais: -->

<!-- - Enquanto há um homem solteiro $h$ -->
<!--     - $h$ propõe à próxima mulher de sua preferência à qual ainda não propôs, $m$ -->
<!--     - Se $m$ está solteira -->
<!--         - $h$ casa com $m$ -->
<!--     - Se não -->
<!--         - Se ela prefere $h$ ao seu parceiro atual $h'$ -->
<!--             - $m$ dispensa $h$, que fica solteiro, e $m$ casa com $h'$ -->




<!-- Para que cada um dos passos seja em tempo constante, é criada uma estrutura de lista para abrigar os homens solteiros e uma matriz para armazenar a posição dos homens no ranking de preferência de cada mulher. A lista possibilita que as operações de inserção e retirada de um solteiro ocorram em $O(1)$. A matriz com o ranking dos homens possibilita que a comparação do parceiro atual da mulher com o homem solteiro candidato ocorra em $O(1)$ também. Com isso, a complexidade do algoritmo é $O(n^2)$. -->



<!-- \section{Avaliação da execução do algoritmo} -->

<!-- A execução do algoritmo foi avaliada com três tipos de instâncias para 22 diferentes valores de n (número de elementos em cada grupo). -->

<!-- $$n = \{ 5, 50, 100, 200, ..., 1000, 1500, ..., 3000, 4000, ..., 9000 \}$$ -->

<!-- Os 3 tipos de instância são os seguintes:  -->

<!-- - As que levam ao melhor caso de execução, ou seja, que exigem o menor número possível de passos, para um valor de $n$; -->

<!-- - As que levam ao pior caso de execução, ou seja, que exigem o maior número possível de passos, para um valor de $n$; -->

<!-- - Instâncias geradas aleatoriamente. -->


<!-- O programa usado para realizar a avaliação empírica do algoritmo foi desenvolvido na linguagem C++, usando apenas as bibliotecas stdio.h e stdlib.h, para leitura e escrita em arquivos e no console, e chrono, para executar a medição dos tempos de execução. Para medição dos tempos de execução foi usado um computador com processador Intel i7-7700 com 3,6 GHz, sistema operacional Windows 10 64 bits. O programa foi compilado no Visual Studio 2017 sem configurações extras de otimização de velocidade. -->

<!-- As instâncias adicionais foram geradas com a linguagem R, que também foi usada para geração deste relatório, juntamente com as bibliotecas tidyverse e RMarkdown. -->




<!-- \subsection{Melhor caso} -->


<!-- Na avaliação do melhor caso, foram usadas instâncias específicas, uma para cada valor de $n$. O algoritmo foi rodado 30 vezes para cada uma das instâncias. -->

<!-- A Figura \ref{fig:melhor_passos} mostra o número de passos do loop princpal do algoritmo (após a preparação das estruturas). É possível ver que o número de passos neste melhor caso é exatamente igual ao número de elementos. -->


<!-- A Figura \ref{fig:melhor_tempo} mostra o tempo em segundos pra cada número de elementos, nas 30 execuções. É possível perceber que o tempo aumenta de forma não linear. A Tabela \ref{reg:tempomelhor} mostra os coeficientes de uma regressão no formato $tempo = \alpha + n^\beta$, que foi estimada para $n > 1000$. É possível ver que o coeficiente $\beta$ se aproxima de 2. O tempo da ordem de $O(n^2)$ é justificado porque antes do loop há a inicialização da estrutura do ranking dos homens, que é feito em $O(n^2)$ passos. Como o número de passos mais pesado do loop principal é pequeno, estes passos de inicialização acabam dominando a execução. A Tabela \ref{tab:melhorcaso} mostra os dados deste melhor caso. -->



<!-- ```{r message=FALSE, warning=FALSE, paged.print=FALSE} -->
<!-- resultado0 <- read_csv("C:\\resultadoapa\\rodada_best_worse_1_a_30.csv", col_names = c("n", "tempo", "passos")) %>%  -->
<!--     mutate(um = 1) %>%  -->
<!--     mutate(id = cumsum(um)) %>%  -->
<!--     filter(id %% 2 == 0, id <= 395)  -->


<!-- resultado1 <- read_csv("C:\\resultadoapa\\rodada_best_worse_10_a_30.csv", col_names = c("n", "tempo", "passos")) %>%  -->
<!--     mutate(um = 1) %>%  -->
<!--     mutate(id = cumsum(um)) %>%  -->
<!--     filter(id %% 2 == 0, id <= 395)  -->

<!-- resultado2 <- read_csv("C:\\resultadoapa\\rodada_best_worse_19_a_30.csv", col_names = c("n", "tempo", "passos")) %>%  -->
<!--     mutate(um = 1) %>%  -->
<!--     mutate(id = cumsum(um)) %>%  -->
<!--     filter(id %% 2 == 0, id <= 395)  -->


<!-- resultado3 <- read_csv("C:\\resultadoapa\\rodada_best_worse_27_a_30.csv", col_names = c("n", "tempo", "passos")) %>%  -->
<!--     mutate(um = 1) %>%  -->
<!--     mutate(id = cumsum(um)) %>%  -->
<!--     filter(id %% 2 == 0, id <= 395)  -->


<!-- resultado <- bind_rows(resultado0, resultado1, resultado2, resultado3) %>%  -->
<!--     mutate(id = cumsum(um)-1) %>%  -->
<!--     mutate(rodada = id %/% 22 ) -->
<!-- ``` -->


<!-- ```{r figs2, fig.cap="\\label{fig:melhor_passos}Número de passos x número de elementos em cada grupo", paged.print=FALSE } -->
<!-- ggplot(resultado) + -->
<!--     geom_line(aes(x = n, y = passos, group = rodada )) + -->
<!--     geom_point(aes(x = n, y = passos, group = rodada )) + -->
<!--     scale_x_continuous(breaks= c(0,1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000), expand = expand_scale(0), limits = c(0,10000)) + -->
<!--     scale_y_continuous(breaks= c(0,1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000), limits = c(0,10000), expand = expand_scale(0)) -->
<!-- ``` -->


<!-- ```{r figs, fig.cap="\\label{fig:melhor_tempo}Tempo de execução (segundos) x número de elementos em cada grupo", message=FALSE, warning=FALSE, paged.print=FALSE} -->
<!-- ggplot(resultado) + -->
<!--     geom_line(aes(x = n, y = tempo, group = rodada )) + -->
<!--     geom_point(aes(x = n, y = tempo, group = rodada )) + -->
<!--     geom_smooth(aes(x = n, y = tempo)) +      -->
<!--     scale_x_continuous(breaks= c(0,1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000), expand = expand_scale(0), limits = c(0,10000)) + -->
<!--     labs(y = "tempo(s)") -->







<!-- ``` -->

<!-- \newpage -->

<!-- ```{r results='asis'} -->



<!-- modelo <-  lm(log(tempo) ~ log(n), data = resultado %>% filter (n> 1000) ) -->

<!-- texreg(modelo, custom.model.names = c("Regressão"), caption = "Regressão log-log: $tempo = \\alpha + n^\\beta$ ", custom.coef.names = c("$\\alpha$","$\\beta$"), label = "reg:tempomelhor") -->

<!-- ``` -->





<!-- ```{r results='asis'} -->

<!-- tabela <- resultado %>%  -->
<!--     group_by(n) %>%  -->
<!--     summarise("Tempo médio (ms)" = mean(tempo*1000), "Passos médios" = as.integer(mean(passos)), "Tempo mediano (ms)" = median(tempo*1000), "Passos medianos" = as.integer(median(passos))) -->

<!-- options(xtable.comment = FALSE) -->
<!-- xtable(tabela, caption = "Dados das 30 execuções do melhor caso", label = "tab:melhorcaso" ) -->


<!-- ``` -->




<!-- \subsection{Pior caso} -->


<!-- Na avaliação do pior caso, foram usadas instâncias específicas, uma para cada valor de $n$. O algoritmo foi rodado 30 vezes para cada uma das instâncias. -->

<!-- A Figura \ref{fig:pior_passos} mostra o número de passos do loop princpal do algoritmo (após a preparação das estruturas). É possível ver que o número de passos neste pior caso evolui de forma não linear. A Tabela \ref{reg:tempomelhor} mostra os coeficientes de uma regressão no formato $tempo = \alpha + n^\beta$, que foi estimada para $n > 1000$. É possível ver que o coeficiente $\beta$ é exatamente 2, sugerindo complexidade $O(n^2)$. -->


<!-- A Figura \ref{fig:pior_tempo} mostra o tempo em segundos pra cada número de elementos, nas 30 execuções. É possível perceber que o tempo aumenta de forma não linear. A Tabela \ref{reg:tempopior} mostra os coeficientes de uma regressão no formato $tempo = \alpha + n^\beta$, que foi estimada para $n > 1000$. É possível ver que o coeficiente $\beta$ é igual de 2, sugerindo complexidade $O(n^2)$. O coeficiente $\alpha$, porém, é menos negativo, o que quer dizer que o tempo de execução é maior neste caso. Realmente podemos observar que o tempo de execução é bem maior comparando as Tabelas \ref{tab:piorcaso} e  \ref{tab:melhorcaso}. -->




<!-- ```{r message=FALSE, warning=FALSE, paged.print=FALSE} -->
<!-- resultadop0 <- read_csv("C:\\resultadoapa\\rodada_best_worse_1_a_30.csv", col_names = c("n", "tempo", "passos")) %>%  -->
<!--     mutate(um = 1) %>%  -->
<!--     mutate(id = cumsum(um)) %>%  -->
<!--     filter(id %% 2 == 1, id <= 395)  -->


<!-- resultadop1 <- read_csv("C:\\resultadoapa\\rodada_best_worse_10_a_30.csv", col_names = c("n", "tempo", "passos")) %>%  -->
<!--     mutate(um = 1) %>%  -->
<!--     mutate(id = cumsum(um)) %>%  -->
<!--     filter(id %% 2 == 1, id <= 395)  -->

<!-- resultadop2 <- read_csv("C:\\resultadoapa\\rodada_best_worse_19_a_30.csv", col_names = c("n", "tempo", "passos")) %>%  -->
<!--     mutate(um = 1) %>%  -->
<!--     mutate(id = cumsum(um)) %>%  -->
<!--     filter(id %% 2 == 1, id <= 395)  -->


<!-- resultadop3 <- read_csv("C:\\resultadoapa\\rodada_best_worse_27_a_30.csv", col_names = c("n", "tempo", "passos")) %>%  -->
<!--     mutate(um = 1) %>%  -->
<!--     mutate(id = cumsum(um)) %>%  -->
<!--     filter(id %% 2 == 1, id <= 395)  -->


<!-- resultadop <- bind_rows(resultadop0, resultadop1, resultadop2, resultadop3) %>%  -->
<!--     mutate(id = cumsum(um)-1) %>%  -->
<!--     mutate(rodada = id %/% 22 ) -->
<!-- ``` -->


<!-- ```{r figs3, fig.cap="\\label{fig:pior_passos}Número de passos x número de elementos em cada grupo", paged.print=FALSE } -->
<!-- ggplot(resultadop) + -->
<!--     geom_line(aes(x = n, y = passos, group = rodada )) + -->
<!--     geom_point(aes(x = n, y = passos, group = rodada )) + -->
<!--     geom_smooth(formula = y ~ poly(x, 2, raw = TRUE), method= "lm", aes(x = n, y = passos)) +      -->
<!--     scale_x_continuous(breaks= c(0,1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000), expand = expand_scale(0), limits = c(0,10000))  -->
<!-- ``` -->


<!-- ```{r figs10, fig.cap="\\label{fig:pior_tempo}Tempo de execução (segundos) x número de elementos em cada grupo", message=FALSE, warning=FALSE, paged.print=FALSE} -->
<!-- ggplot(resultadop) + -->
<!--     geom_line(aes(x = n, y = tempo, group = rodada )) + -->
<!--     geom_point(aes(x = n, y = tempo, group = rodada )) + -->
<!--     geom_smooth(formula = y ~ poly(x, 2, raw = TRUE), method= "lm", aes(x = n, y = tempo)) +      -->
<!--     scale_x_continuous(breaks= c(0,1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000), expand = expand_scale(0), limits = c(0,10000)) + -->
<!--     labs(y = "tempo(s)") -->







<!-- ``` -->

<!-- \newpage -->


<!-- ```{r results='asis'} -->



<!-- modelopp <-  lm(log(passos) ~ log(n) , data = resultadop %>%  filter(n > 1000)  ) -->

<!-- texreg(modelopp, custom.model.names = c("Regressão"), caption = "Regressão log-log: $passos = \\alpha + n^\\beta$ ", custom.coef.names = c("$\\alpha$","$\\beta$"), label = "reg:passospior") -->

<!-- ``` -->



<!-- ```{r results='asis'} -->



<!-- modelopt <-  lm(log(tempo) ~ log(n) , data = resultadop %>%  filter(n > 1000)  ) -->

<!-- texreg(modelopt, custom.model.names = c("Regressão"), caption = "Regressão log-log: $tempo = \\alpha + n^\\beta$ ", custom.coef.names = c("$\\alpha$","$\\beta$"), label = "reg:tempopior") -->

<!-- ``` -->




<!-- ```{r results='asis'} -->

<!-- tabela <- resultadop %>%  -->
<!--     group_by(n) %>%  -->
<!--     summarise("Tempo médio (ms)" = mean(tempo*1000), "Passos médios" = as.integer(mean(passos)), "Tempo mediano (ms)" = median(tempo*1000), "Passos medianos" = as.integer(median(passos))) -->

<!-- options(xtable.comment = FALSE) -->

<!-- xtable(tabela, caption = "Dados das 30 execuções do pior caso", label = "tab:piorcaso", -->

<!--        display = c("d","d","f","e","f","e")  -->
<!--        ) -->


<!-- ``` -->


<!-- \subsection{Caso médio} -->


<!-- Na avaliação do caso médio, foram usadas 30 instâncias geradas aleatoriamente para cada valor de $n$. O algoritmo foi rodado uma vez para cada uma das instâncias, gerando 30 execuções para cada valor de n. Neste caso, portanto, diferentemente dos anteriores, temos uma variação no número de passos, que passa a não ser mais determinístico. -->

<!-- A Figura \ref{fig:ale_passos} mostra o número de passos do loop principal do algoritmo (após a preparação das estruturas). É possível ver que o número de passos neste pior caso evolui de forma quase linear. A Tabela \ref{reg:passosale} mostra os coeficientes de uma regressão no formato $tempo = \alpha + n^\beta$, que foi estimada para $n > 1000$. É possível ver que o coeficiente $\beta$ está perto 1,1, sugerindo complexidade do loop principal está perto de $O(n)$ para o caso médio. -->


<!-- A Figura \ref{fig:ale_tempo} mostra o tempo em segundos pra cada número de elementos, nas 30 execuções. A Tabela \ref{reg:passosale} relativa à regressão do tempo mostra um coeficiente $\beta$ perto de 2 É possível perceber que o tempo aumenta de forma não linear. É possível reparar, também, olhando as Tabelas \ref{tab:melhorcaso}, \ref{tab:piorcaso} e \ref{tab:ale}, que o tempo de execução do caso médio está mais perto do tempo do melhor caso do que o do pior caso. -->




<!-- ```{r message=FALSE, warning=FALSE, paged.print=FALSE} -->

<!-- resultadoale1 <- read_csv("C:\\resultadoapa\\rodada_ale_1_a_7.csv", col_names = c("n", "tempo", "passos")) -->

<!-- resultadoale2 <- read_csv("C:\\resultadoapa\\rodada_ale_8_a_15.csv", col_names = c("n", "tempo", "passos")) -->

<!-- resultadoale3 <- read_csv("C:\\resultadoapa\\rodada_ale_16_a_22.csv", col_names = c("n", "tempo", "passos")) -->

<!-- resultadoale4 <- read_csv("C:\\resultadoapa\\rodada_ale_23_a_30.csv", col_names = c("n", "tempo", "passos")) -->

<!-- resultadoale <- bind_rows(resultadoale1, resultadoale2, resultadoale3, resultadoale4) %>%  -->
<!--     mutate(um = 1) %>%  -->
<!--     mutate(id = cumsum(um)-1) %>%  -->
<!--     mutate(rodada = id %/% 22 ) -->

<!-- ``` -->


<!-- ```{r figs9, fig.cap="\\label{fig:ale_passos}Número de passos x número de elementos em cada grupo", message=FALSE, warning=FALSE, paged.print=FALSE} -->
<!-- ggplot(resultadoale) + -->
<!--     geom_line(aes(x = n, y = passos, group = rodada )) + -->
<!--     geom_point(aes(x = n, y = passos, group = rodada )) + -->
<!--     geom_smooth( aes(x = n, y = passos)) +      -->
<!--     scale_x_continuous(breaks= c(0,1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000), expand = expand_scale(0), limits = c(0,10000))  -->
<!-- ``` -->


<!-- ```{r figs4, fig.cap="\\label{fig:ale_tempo}Tempo de execução (segundos) x número de elementos em cada grupo", message=FALSE, warning=FALSE, paged.print=FALSE} -->
<!-- ggplot(resultadoale) + -->
<!--     geom_line(aes(x = n, y = tempo, group = rodada )) + -->
<!--     geom_point(aes(x = n, y = tempo, group = rodada )) + -->
<!--     geom_smooth(formula = y ~ poly(x, 2, raw = TRUE), method= "lm", aes(x = n, y = tempo)) +      -->
<!--     scale_x_continuous(breaks= c(0,1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000), expand = expand_scale(0), limits = c(0,10000)) + -->
<!--     labs(y = "tempo(s)") -->







<!-- ``` -->

<!-- \newpage -->

<!-- ```{r message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'} -->



<!-- modeloalep <-  lm(log(passos) ~ log(n) , data = resultadoale %>%  filter(n > 1000)  ) -->

<!-- texreg(modeloalep, custom.model.names = c("Regressão"), caption = "Regressão log-log: $passos = \\alpha + n^\\beta$ ", custom.coef.names = c("$\\alpha$","$\\beta$"), label = "reg:passosale") -->

<!-- ``` -->



<!-- ```{r results='asis'} -->



<!-- modeloalet <-  lm(log(tempo) ~ log(n) , data = resultadoale %>%  filter(n > 1000)  ) -->

<!-- texreg(modeloalet, custom.model.names = c("Regressão"), caption = "Regressão log-log: $tempo = \\alpha + n^\\beta$ ", custom.coef.names = c("$\\alpha$","$\\beta$"), label = "reg:tempoale") -->

<!-- ``` -->




<!-- ```{r results='asis'} -->

<!-- tabela <- resultadoale %>%  -->
<!--     group_by(n) %>%  -->
<!--     summarise("Tempo médio (ms)" = mean(tempo*1000), "Passos médios" = as.integer(mean(passos)), "Tempo mediano (ms)" = median(tempo*1000), "Passos medianos" = as.integer(median(passos))) -->

<!-- options(xtable.comment = FALSE) -->

<!-- xtable(tabela, caption = "Dados das 30 execuções do pior caso", label = "tab:ale", -->

<!--        display = c("d","d","f","e","f","e")  -->
<!--        ) -->


<!-- ``` -->


<!-- \section{Conclusões} -->


<!-- Pudemos atestar que, conforme o esperado, a complexidade do algoritmo se mostra $O(n^2)$ na análise empírica. O tempo de execução segue $O(n^2)$ mesmo no melhor caso e no pior caso, apesar de a execução ser em tempos muito menores do que nas instâncias de pior caso. Isso acontece porque existe uma inicialização antes do loop principal que tem complexidade $O(n^2)$ mesmo no melhor caso e no caso médio. Esta inicialização acaba prevalecendo na análise do tempo asintótica em relação a $n$. -->

<!-- A descoberta maisi interessante foi o ffato de que a execução do caso médio está muito mais perto do melhor caso do que do pior caso tanto em número de passos quanto em tempo de execução. -->





